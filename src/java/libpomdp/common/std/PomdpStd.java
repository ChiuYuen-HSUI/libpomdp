/** ------------------------------------------------------------------------- *
 * libpomdp
 * ========
 * File: 
 * Description: Represent a POMDP model using a flat representation and
 *              sparse matrices and vectors. This class can be constructed
 *              from a pomdpSpecSparseMTJ object after parsing a .pomdp file.
 *              Sparse matrices by matrix-toolkits-java, 
 *              every matrix will be CustomMatrix:
 *              
 * S =
 *  (3,1)        1
 *  (2,2)        2
 *  (3,2)        3
 *  (4,3)        4
 *  (1,4)        5
 * A =
 *   0     0     0     5
 *   0     2     0     0
 *   1     3     0     0
 *   0     0     4     0
 * Copyright (c) 2009, 2010, 2011 Diego Maniloff
 * Copyright (c) 2010, 2011 Mauricio Araya
 --------------------------------------------------------------------------- */

package libpomdp.common.std;

// imports
import java.io.Serializable;

import libpomdp.common.AlphaVector;
import libpomdp.common.BeliefMdp;
import libpomdp.common.BeliefState;
import libpomdp.common.CustomMatrix;
import libpomdp.common.CustomVector;
import libpomdp.common.ObservationModel;
import libpomdp.common.Pomdp;
import libpomdp.common.RewardFunction;
import libpomdp.common.TransitionModel;
import libpomdp.common.Utils;

public class PomdpStd implements Pomdp, Serializable {

    /**
     * Generated by Eclipse.
     */
    private static final long serialVersionUID = -5511401938934887929L;

    // ------------------------------------------------------------------------
    // properties
    // ------------------------------------------------------------------------

    // number of states
    protected int nrSta;

    // private nrAct
    protected int nrAct;

    // private nrObs
    protected int nrObs;

    // transition model: a x s x s'
    protected TransitionModelStd T;

    // observation model: a x s' x o
    protected ObservationModelStd O;

    // reward model: a x s'
    protected RewardFunctionStd R;

    // discount factor
    protected double gamma;

    // action names
    protected String actStr[];

    // observation names
    protected String obsStr[];

    // state names
    protected String staStr[];

    // starting belief
    protected BeliefStateStd initBelief;

    // ------------------------------------------------------------------------
    // methods
    // ------------------------------------------------------------------------

    // / constructor
    public PomdpStd(CustomMatrix[] O, CustomMatrix[] T, CustomVector[] R,
	    int nrSta, int nrAct, int nrObs, double gamma, String staStr[],
	    String actStr[], String obsStr[], CustomVector init) {

	// allocate space for the pomdp models
	this.nrSta = nrSta;
	this.nrAct = nrAct;
	this.nrObs = nrObs;
	this.gamma = gamma;
	this.actStr = actStr;
	this.obsStr = obsStr;

	// set initial belief state
	this.initBelief = new BeliefStateStd(init, 0.0);

	// copy the model matrices - transform from dense to comprow
	// do we really need this? dense is in sparse form already...
	for (int a = 0; a < nrAct; a++) {
	    this.T = new TransitionModelStd(T);
	    this.O = new ObservationModelStd(O);
	    this.R = new RewardFunctionStd(R);
	}
    } // constructor

    public PomdpStd(PomdpStd pomdp) {
	this.nrSta = pomdp.nrSta;
	this.nrAct = pomdp.nrAct;
	this.nrObs = pomdp.nrObs;
	this.T = pomdp.T;
	this.O = pomdp.O;
	this.R = pomdp.R;
	this.gamma = pomdp.gamma;
	this.staStr = pomdp.staStr;
	this.actStr = pomdp.actStr;
	this.obsStr = pomdp.obsStr;
	this.initBelief = pomdp.initBelief;
    }

    // / tao(b,a,o)
    
    public BeliefState nextBeliefState(BeliefState b, int a, int o) {
	// long start = System.currentTimeMillis();
	// System.out.println("made it to tao");
	BeliefState bPrime;
	// compute T[a] * b
	CustomVector v = T.project(b.getPoint(),a);
	// System.out.println("Elapsed in tao - T[a] * b" +
	// (System.currentTimeMillis() - start));

	// element-wise product with O[a](:,o)
	v.elementMult(O.getRow(o,a));
	// System.out.println("Elapsed in tao - O[a] .* b2" +
	// (System.currentTimeMillis() - start));

	// compute P(o|b,a) - norm1 is the sum of the absolute values
	double poba = v.norm(1.0);
	// make sure we can normalize
	if (poba < 0.00001) {
	    // System.err.println("Zero prob observation - resetting to init");
	    // this branch will have poba = 0.0
	    bPrime = initBelief;
	} else {
	    // safe to normalize now
	    v.normalize();
	    bPrime = new BeliefStateStd(v, poba);
	}
	// System.out.println("Elapsed in tao" + (System.currentTimeMillis() -
	// start));
	// return
	return bPrime;
    }

    /// R(b,a)
    
    public double expectedImmediateReward(BeliefState b, int a) {
	return R.getExpectation(b,a);
    }

    // P(o|b,a) in vector form for all o's

    public CustomVector observationProbabilities(BeliefState b, int a) {
	CustomVector Tb = T.project(b.getPoint(),a);
	CustomVector Poba = O.project(Tb,a);
	Poba.normalize();
	return Poba;
    }
    
    public BeliefState getInitialBeliefState() {
	return initBelief.copy();
    }

    
    public int nrStates() {
	return nrSta;
    }

    
    public int nrActions() {
	return nrAct;
    }

    
    public int nrObservations() {
	return nrObs;
    }

    
    public double getGamma() {
	return gamma;
    }

    
    public String getActionString(int a) {
	return actStr[a];
    }

    
    public String getObservationString(int o) {
	return obsStr[o];
    }

    
    public String getStateString(int s) {
	return staStr[s];
    }

    public int getRandomAction() {
        return (Utils.gen.nextInt(Integer.MAX_VALUE) % nrActions());
    }

    public int sampleObservation(BeliefState b, int a) {
	double roulette = Utils.gen.nextDouble();
	CustomVector vect=observationProbabilities(b,a);
	double sum = 0.0;
	for (int o = 0; o < nrObs; o++) {
	    sum += vect.get(o);
	    if (roulette < sum)
		return o;
	}
	return (-1);
    }

    public AlphaVectorStd mdpValueUpdate(AlphaVector alpha, int a) {
	CustomVector vec = T.project((CustomVector)alpha.getInternalRef(),a);
	vec.scale(gamma);
	vec.add((CustomVector)getRewardValueFunction(a).getAlpha().getInternalRef());
	return (new AlphaVectorStd(vec, a));
    }

    public ValueFunctionStd getRewardValueFunction(int a) {
	ValueFunctionStd vf = new ValueFunctionStd();
	vf.push(R.getVector(a), a);
	return vf;
    }

    public double getRewardMax() {
	double max_val = Double.NEGATIVE_INFINITY;
	for (int a = 0; a < nrActions(); a++) {
	    double test_val = getRewardMax(a);
	    if (test_val > max_val)
		max_val = test_val;
	}
	return max_val;
    }

    public double getRewardMin() {
	double min_val = Double.POSITIVE_INFINITY;
	for (int a = 0; a < nrActions(); a++) {
	    double test_val = getRewardMin(a);
	    if (test_val < min_val)
		min_val = test_val;
	}
	return min_val;
    }

    public double getRewardMaxMin() {
	double max_val = Double.NEGATIVE_INFINITY;
	for (int a = 0; a < nrActions(); a++) {
	    double test_val = getRewardMin(a);
	    if (test_val > max_val)
		max_val = test_val;
	}
	return max_val;
    }

    public double getRewardMin(int a) {
	return (R.min(a));
    }

    public double getRewardMax(int a) {
	return (R.max(a));
    }

	public AlphaVector getEmptyAlpha() {
		return new AlphaVectorStd(nrSta);
	}

	public BeliefMdp getBeliefMdp() {
		return new BeliefMdpStd(this);
	}
	
	public AlphaVector getHomogeneAlpha(double bestVal) {
		return (new AlphaVectorStd(CustomVector.getHomogene(nrStates(),
				bestVal), -1));
	}

	public AlphaVector getEmptyAlpha(int a) {
		return (new AlphaVectorStd(nrStates(), a));
	}

	public ObservationModel getObservationModel() {
		return O;
	}

	public RewardFunction getRewardFunction() {
		return R;
	}

	public TransitionModel getTransitionModel() {
		return T;
	}



} // PomdpStd.java

